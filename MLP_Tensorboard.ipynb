{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  Let the Tensors Board?"
      ],
      "metadata": {
        "id": "i55uKXJ1M7wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPFvA-XWGrl8",
        "outputId": "389a6298-fb83-4fde-cc21-cddfe8356814"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"drive/MyDrive/Colab Notebooks/IDL\")"
      ],
      "metadata": {
        "id": "UXNqKYsv5-9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import MNISTDataset\n",
        "%load_ext tensorboard\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "hqNhyloy6ANz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp1YQ0ERjhWA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "d7dfef3f-391b-426e-ae3f-bdddae414327"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# we can look at any of the images and the corresponding labels\n",
        "# say, image no. 155\n",
        "plt.imshow(train_images[155], cmap=\"Greys_r\")\n",
        "plt.show()\n",
        "print(train_labels[155])\n",
        "\n",
        "train_images = (train_images.astype(np.float32) / 255.).reshape((-1, 784))\n",
        "test_images = (test_images.astype(np.float32) / 255.).reshape((-1, 784))\n",
        "\n",
        "train_labels = train_labels.astype(np.int32)\n",
        "test_labels = test_labels.astype(np.int32)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd08r9bu81lw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "279064f2-c75c-4228-d8ee-16feaaba79c6"
      },
      "source": [
        "# 60000 elements in the training set\n",
        "print(train_images.shape, train_labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784) (60000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi Layer Perceptron"
      ],
      "metadata": {
        "id": "WhGi2SYUwBzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Dynamic Layers and Activation Functions"
      ],
      "metadata": {
        "id": "LYoR2PDXyXpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **init_params()** Initializes all the weights and biases \n",
        "*   **forward_propogation()** computes return the outputs given W and b\n",
        "*   **backwardpass()** computes the gradient based on the error from the forward pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pYoJMsQ-wLcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def init_params(minval,maxval,num_layers):\n",
        "  params = {}\n",
        "  for i in range(len(num_layers)-1):\n",
        "    params[\"w_h\"+str(i)] = tf.Variable(tf.random.uniform([num_layers[i],num_layers[i+1]],minval=minval,maxval=maxval))\n",
        "    params[\"b_h\"+str(i)] = tf.Variable(tf.random.uniform([num_layers[i+1]],minval=minval,maxval=maxval))\n",
        "\n",
        "  return params"
      ],
      "metadata": {
        "id": "4ZeoLmQk_mlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_activation_functions(activation_fn,x):\n",
        "  if activation_fn==\"sigmoid\":\n",
        "    return tf.nn.sigmoid(x)\n",
        "  elif activation_fn==\"relu\":\n",
        "    return tf.nn.relu(x)\n",
        "  elif activation_fn==\"tanh\":\n",
        "    return tf.nn.tanh(x)\n"
      ],
      "metadata": {
        "id": "pt1-1jeVUODT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propogation(inputs,num_layers,params,activation_fn):\n",
        "\n",
        "  '''\n",
        "  inputs = image_batch\n",
        "  num_layers = size of the layers inlcuding i/p and o/p\n",
        "  params = weights and biases of num_layers-1\n",
        "  activation_fn = \n",
        "  '''\n",
        "\n",
        "  layer_outputs = {}\n",
        "  for i in range(num_layers-1):\n",
        "    if i==0: #input\n",
        "      layer_outputs[\"h_\"+str(i)]= tf.add(tf.matmul(inputs,params[\"w_h\"+str(i)]),params[\"b_h\"+str(i)])\n",
        "      layer_outputs[\"h_\"+str(i)] = apply_activation_functions(activation_fn,layer_outputs[\"h_\"+str(i)])\n",
        "    elif (i>0) and (i!=num_layers-2): # hidden\n",
        "\n",
        "      layer_outputs[\"h_\"+str(i)]= tf.add(tf.matmul(layer_outputs[\"h_\"+str(i-1)],params[\"w_h\"+str(i)]),params[\"b_h\"+str(i)])\n",
        "      layer_outputs[\"h_\"+str(i)] = apply_activation_functions(activation_fn,layer_outputs[\"h_\"+str(i)])\n",
        "\n",
        "    elif i==num_layers-2:#output\n",
        "      layer_outputs[\"output\"]= tf.add(tf.matmul(layer_outputs[\"h_\"+str(i-1)],params[\"w_h\"+str(i)]),params[\"b_h\"+str(i)])\n",
        "\n",
        "  return layer_outputs[\"output\"]"
      ],
      "metadata": {
        "id": "A57D9DJYEMPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_pass(image_batch,label_batch,learning_rate,num_layers,params,activation_fn):\n",
        "  '''\n",
        "  image_batch = image_batch\n",
        "  label_batch = label_batch\n",
        "  learning_rate = learning rate\n",
        "  num_layers = from forward pass\n",
        "  params = from forward pass\n",
        "  activation_fn = name of the activation function\n",
        "  '''\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = forward_propogation(inputs=image_batch,num_layers=num_layers,params=params,activation_fn=activation_fn)\n",
        "    xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=logits, labels=label_batch))\n",
        "    grads = tape.gradient(xent,list(params.values()))\n",
        "\n",
        "    for i in range(0,len(grads),2):\n",
        "      params[\"w_h\"+str(i//2)].assign_sub(learning_rate * grads[i])\n",
        "      params[\"b_h\"+str(i//2)].assign_sub(learning_rate * grads[i+1])\n",
        "    return logits, xent, params\n",
        "    "
      ],
      "metadata": {
        "id": "Z9v-MNySEiUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_model(train_data,train_steps,learning_rate,num_layers,param_list,activation_fn,log_name):\n",
        "\n",
        "  logdir = os.path.join(\"logs\", log_name + str(datetime.now()))\n",
        "  train_writer = tf.summary.create_file_writer(os.path.join(logdir, \"train\"))\n",
        "  test_writer = tf.summary.create_file_writer(os.path.join(logdir, \"test\"))\n",
        "  params = param_list\n",
        "  batch_loss_over_iter = []\n",
        "  acc_over_iter = []\n",
        "  step = 0\n",
        "  while step<=train_steps:\n",
        "    for image_batch, label_batch in train_data:\n",
        "      \n",
        "      if step>train_steps:\n",
        "        break\n",
        "      step+=1\n",
        "      logits, xent, params = backward_pass(image_batch,label_batch,learning_rate,len(num_layers),params,activation_fn)\n",
        "      preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "      acc = tf.reduce_mean(tf.cast(tf.equal(preds, label_batch),\n",
        "                              tf.float32))\n",
        "\n",
        "      if not step % 10:\n",
        "        \n",
        "        test_logits = forward_propogation(inputs=test_images,num_layers=len(num_layers),params=params,activation_fn=activation_fn)\n",
        "        test_preds = tf.argmax(test_logits, axis=1, output_type=tf.int32)\n",
        "        test_acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, test_labels),\n",
        "                             tf.float32))\n",
        "        # batch_loss_over_iter.append(xent.numpy())\n",
        "        # acc_over_iter.append(acc.numpy())\n",
        "       \n",
        "        with train_writer.as_default():\n",
        "          tf.summary.scalar(\"accuracy\",acc,step=step)\n",
        "          tf.summary.histogram(\"logits\", logits, step=step)\n",
        "          tf.summary.histogram(\"weights_input\", params[\"w_h0\"], step=step)\n",
        "          tf.summary.image(\"input\", tf.reshape(image_batch, [-1, 28, 28, 1]), step=step)\n",
        "\n",
        "        with test_writer.as_default():\n",
        "          tf.summary.scalar(\"accuracy\",test_acc,step=step)\n",
        "      \n",
        "      if not step % 1000:\n",
        "          print(\"Step {}. Batch loss: {} Batch accuracy: {}\".format(step+1, xent, acc))\n",
        "  return params, batch_loss_over_iter, acc_over_iter\n"
      ],
      "metadata": {
        "id": "pqVoSPr3JBTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Shuffle Batch Repeat"
      ],
      "metadata": {
        "id": "cEp0B9Jt0OJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "train_steps = 2000\n",
        "learning_rate = 0.1\n",
        "num_layers=[784,256,10]\n",
        "params = init_params(-0.1,0.1,num_layers)\n",
        "activation_fn = \"sigmoid\"\n",
        "\n",
        "train_data_ = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "test_data = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
        "\n",
        "print(\"Shuffle-Batch-Repeat\")\n",
        "train_data = train_data_.shuffle(60000).batch(128).repeat(3)\n",
        "updated_params, _ , _ = train_model(train_data, train_steps,learning_rate,num_layers,params,activation_fn,\"Shuffle-Batch-Repeat\")\n",
        "\n",
        "print(\"Shuffle-Repeat-Batch\")\n",
        "train_data = train_data_.shuffle(60000).repeat(3).batch(128)\n",
        "updated_params, _ , _ = train_model(train_data, train_steps,learning_rate,num_layers,params,activation_fn,\"Shuffle-Repeat-Batch\")\n",
        "\n",
        "print(\"Batch-Repeat-Shuffle\")\n",
        "train_data = train_data_.batch(128).repeat(3).shuffle(60000)\n",
        "updated_params, _ , _ = train_model(train_data, train_steps,learning_rate,num_layers,params,activation_fn,\"Batch-Repeat-Shuffle\")\n",
        "\n",
        "print(\"Batch-Shuffle-Repeat\")\n",
        "train_data = train_data_.batch(128).shuffle(60000).repeat(3)\n",
        "updated_params, _ , _ = train_model(train_data, train_steps,learning_rate,num_layers,params,activation_fn,\"Batch-Shuffle-Repeat\")\n",
        "\n",
        "print(\"Repeat-Shuffle-Batch\")\n",
        "train_data = train_data_.repeat(3).shuffle(60000).batch(128)\n",
        "updated_params, _ , _ = train_model(train_data, train_steps,learning_rate,num_layers,params,activation_fn,\"Repeat-Shuffle-Batch\")\n",
        "\n",
        "print(\"Repeat-Batch-Shuffle\")\n",
        "train_data = train_data_.repeat(3).batch(128).shuffle(60000)\n",
        "updated_params, _ , _ = train_model(train_data, train_steps,learning_rate,num_layers,params,activation_fn,\"Repeat-Batch-Shuffle\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6W519xypStvM",
        "outputId": "853628b8-3574-4b13-f6ed-46479a4ed4aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shuffle-Batch-Repeat\n",
            "Step 1001. Batch loss: 0.384280264377594 Batch accuracy: 0.921875\n",
            "Step 2001. Batch loss: 0.4040263891220093 Batch accuracy: 0.890625\n",
            "Shuffle-Repeat-Batch\n",
            "Step 1001. Batch loss: 0.3100601136684418 Batch accuracy: 0.8984375\n",
            "Step 2001. Batch loss: 0.12054286152124405 Batch accuracy: 0.9609375\n",
            "Batch-Repeat-Shuffle\n",
            "Step 1001. Batch loss: 0.5125271081924438 Batch accuracy: 0.90625\n",
            "Step 2001. Batch loss: 0.08836393058300018 Batch accuracy: 0.984375\n",
            "Batch-Shuffle-Repeat\n",
            "Step 1001. Batch loss: 0.11751774698495865 Batch accuracy: 0.984375\n",
            "Step 2001. Batch loss: 0.1616492122411728 Batch accuracy: 0.9609375\n",
            "Repeat-Shuffle-Batch\n",
            "Step 1001. Batch loss: 0.12077663838863373 Batch accuracy: 0.96875\n",
            "Step 2001. Batch loss: 0.21420961618423462 Batch accuracy: 0.9296875\n",
            "Repeat-Batch-Shuffle\n",
            "Step 1001. Batch loss: 0.35467761754989624 Batch accuracy: 0.8984375\n",
            "Step 2001. Batch loss: 0.2652748227119446 Batch accuracy: 0.9453125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shuffle method uses a fixed-size buffer to shuffle the items as they pass through. Setting a buffer_size greater than the number of examples in the Dataset ensures that the data is completely shuffled. Batch is used the group the instances into batches and repeat repeates the instances for a specified number of epoches (if specified).\n",
        "\n",
        "\n",
        "## Batch-Repeat-Shuffle\n",
        "This combination creates batches based on the order of the input data, and then repeats the same batches n(count) times. The order of the batches are then shuffled lastly.\n",
        "## Batch-Shuffle-Repeat\n",
        "This combination creates batches based on the order of the input data, and shuffles the order of the batches randomly. Lastly; The shuffled batches are then repeated n(count) times.\n",
        "## Shuffle-Batch-Repeat\n",
        "This combination shuffles the input data randommly firstly. Then it creates batches based on the shuffled data. Lastly The batches are then repeated n(count) times.\n",
        "## Repeat-Batch-Shuffle\n",
        "This combination repeats the input data firstly. Then it creates batches based on the repeated input data data. Lastly The batches are then shuffled randomly.\n",
        "\n",
        "## Shuffle-Repeat-Batch\n",
        "This combination shuffles the input data. Then it repeats the shuffling data n(count) times. Lastly it creates batches of a defined batch_size. This is theoretically better than the combinations above but still can be biased on the initial shuffle.\n",
        "\n",
        "## Repeat-Shuffle-Batch\n",
        "This combination repeats the input data n(count) times. Then it shuffles the repeated input data randomly data n(count) times. Lastly it creates batches of a defined batch_size based on the shuffled, repeated data.\n",
        "\n",
        "## Conclusion \n",
        "It was agreed that the batching should be the last step while training the neural network.This is due to the fact that the batch composition should be different and well distributed for the classes available in the dataset, and this would not be possible if input data was biased.   \n",
        "\n",
        "Repeat-shuffle-batch was agreed to be the winner as the batch composition would be most varied. The order of input instance is arbitrary for each batch and not repeated for any other batch in the dataset."
      ],
      "metadata": {
        "id": "ss8eTGqClYuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%tensorboard --logdir logs"
      ],
      "metadata": {
        "id": "S7bgS0Oj0M14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fail Scripts"
      ],
      "metadata": {
        "id": "4PUefG-HmHGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "83omVY4syEWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# fail 1\n",
        "Loss is nan. Gradient blow up\n",
        "\n",
        "1. Changing the number of layers in the network to 1 seem to improve the accuracy to ~97%. \n",
        "2.  changing the weights to 0.1 resolves the exploding of gradients"
      ],
      "metadata": {
        "id": "tX5kCScfyV3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# fail 2\n",
        "\n",
        "1. Changing the number of layers in the network to 2 seem to improve the accuracy to ~97%. The reason could be that the gradients are vanishing.  \n",
        "2. Increasing the weight range to 1 increases the **accuracy**\n",
        "3. Changing the activation function to relu works maybe becasue the values provided to the sigmoid is so small that it gets saturated."
      ],
      "metadata": {
        "id": "bTRkBvLBmZuh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# fail 3\n",
        "\n",
        "1. Changing the range of weights seems to increase the accuracy \n",
        "2. Relu resticts the weights to positive but we have weights from [-0.1,0]. So changing the activation to leaklyRELU or sigmoid with a higher learnign rate solves the problem. since the weights range lie between [0, -0.1] the RELU outputs 0, and technically there is no learning process and the remains same.\n"
      ],
      "metadata": {
        "id": "MPxX5CYBq8vE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# fail 4\n",
        "\n",
        "1. Changing the stddev of noise to 0.1 worked. The image is too noisy to learn and hence the training accuracy is very low. Too little noise or too much noise, both extremes are probably not a good idea\n",
        "\n",
        "```\n",
        "img_batch += tf.random.normal(tf.shape(img_batch), stddev=0.1)\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZVAK2Z_4sUax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# fail 5\n",
        "\n",
        "we are technically applying softmax on top of softmax which will lead to incorrect results. From my understanding, I think the highest probability label will remain the same. But it could be that the error/loss differs. (may be the number of steps required to converge might be more)"
      ],
      "metadata": {
        "id": "cwjaQO0IujCh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EszESYD1h-Di"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}